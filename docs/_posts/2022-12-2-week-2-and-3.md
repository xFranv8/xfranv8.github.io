---
title: "Week 2 and 3 - Reinforcement Learning in Carla: First Approach"
categories:
  - Weekly Log
tags:
  - reinforcement learning
  - Q-Learning
  - carla
---

These two weeks I've been working on making all of my code Object-Oriented, in order to make it easier for the later implementation of the Q-Learn algorithm.
After that, I started reading [Ignacio Arranz master's thesis](https://gsyc.urjc.es/jmplaza/students/tfm-reinforcementlearning-conduccion_autonoma-ignacio_arranz-2020.pdf),
where he gets to develop a Q-Learn agent that can follow a line in the Gazebo simulator, so the idea is to make a similar implementation of the Q-Learn algorithm, but this time using the Carla Simulator.

First, we need to decide the actions that our model is going to be able to take.
The model can choose between this three actions:
1. Turn left
2. Turn right
3. Straight forward

We also need to define the states in which the agent can be, which will be 17 different states depending on the position of the center of the line in relation to the orientation of the car.
Lastly, we also have to define a reward function, in which we will give a higher reward to the actions that make the agent remain in the center of the line.

When we have decided all of these parameters, we can implement the Q-Learn algorithm, which will update the Q-table _(state, action) --> reward_ with this formula:
-  Q(_state_, _action_) = (1 - α) + α(_reward_) + γ maxQ (_next state_, _all actions_)

In this case we have set the hyperparameters of this formula as:
- α = 0.8
- γ = 0.9

Here you can watch a video that summarizes in a very visual way the agent's training, and how it finally gets to complete the "circuit".
<iframe width="560" height="315" src="https://www.youtube.com/embed/mabmtAHJ0u8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

There are a lot of ways in which we could optimize the agent behaviour.
Adding more states, making a better reward function so that the agent gets to speed up and making the perception more robust are some ways in which we could create a more efficient agent, so this is what I'll be working on the next few weeks.

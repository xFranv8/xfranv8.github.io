---
title: "Practice 3: Autolocalization"
categories:
  - Daily Log
tags:
  - github page
  - 3-dimensional reconstruction
  - gazebo
---

In this practice we were supposed to implement an autolocalization algorithm based on the Perspective N Points (PnP) algorithm. 
The algorithm is based on the following steps:

1. First, we need to calibrate our camera.

    For this purpose, we will use the algorithm that we have already used in another subject of the master (3D Vision). This algorithm requieres a series of images from
    a chessboard taken from different angles.
    
    <figure class="half" style="display: flex;flex-direction: row; justify-content: center;">
      <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/calibration_pattern.svg" alt="" style="width:51%">
    </figure>
    
    In this case, we've used 12 images taken with my phone camera, these images have a resolution of 
    4032x3024 pixels. Here you can watch some examples of the images used for the calibration:
    
    <figure class="half" style="display: flex;flex-direction: row; justify-content: center;">
      <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/IMG_4047.png" alt="" style="width:35%">
      <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/IMG_4049.png" alt="" style="width:35%">
      <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/IMG_4050.png" alt="" style="width:35%">
    </figure>
    
    The class _CalibrateCamera_ implemented in the **CalibrateCamera.py** file receives the directory all the calibration pattern
    images, and with its method _calibrate_, we're able to obtain the camera matrix and the distortion coefficients.
    We can set the parameter _check_ of this class to true if we want to check the calibration results. In this case, the method 
    will show the projections of the coordinate axes in each of the images, as shown in the figure below:
    
    <figure class="half" style="display: flex;flex-direction: row; justify-content: center;">
      <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/checking_calibration1.png" alt="" style="width:35%">
      <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/checking_calibration2.png" alt="" style="width:35%">
      <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/checking_calibration3.png" alt="" style="width:35%">
    </figure>

    As we can see, the calibration results are quite good, so we can use them to obtain the camera pose.

2. Once we have our camera calibrated, we can start with the autolocalization algorithm. In order to do that, we need to generate
an aruco marker, which will be used as a reference in the autolocalization process. We can generate the marker with the _create_marker.py_
file, which will generate a 6x6 ArUco marker as the one shown below:

    <figure class="half" style="display: flex;flex-direction: row; justify-content: center;">
      <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/marker.png" alt="" style="width:50%">
    </figure>

3. After that, we have to detect this ArUco marker in the image. In order to complete this step, the **MarkerDetector** class has been implemented.
This class has a method called _detect_ which receives an image and returns the corners of the marker in the image. In order to detect the marker,
this method uses the _aruco.detectMarkers_ function from the _cv2_ library. The detection of the marker is shown in the figure below:

    <figure class="half" style="display: flex;flex-direction: row; justify-content: center;">
      <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/marker_detected_cropped.png" alt="" style="width:50%">
    </figure>

4. Finally, we are able to estimate the pose of our camera by using the _cv2.solvePnP_ function. This function receives the 3D coordinates of the marker,
the 2D coordinates of the marker in the image, the camera matrix and the distortion coefficients, and returns the rotation and translation vectors of the camera.
With these vectors, we can obtain the rotation matrix and the translation vector of the camera. The center of the camera is obtained as follows:
   
   $$A = K \cdot R$$
   $$B = K \cdot t$$
   $$C = A^{-1} \cdot B$$
    
   Where _K_ is the camera matrix, _R_ is the rotation matrix and _t_ is the translation vector. The center of the camera is the point _C_.
   With this calculation, we've finished the autolocalization process.

5. In order to test the autolocalization algorithm, we've recorded a video with my phone camera, and tested the algorithm with the frames of this video. The position of the camera is represented
in a 3D matplotlib plot. Here you can watch the results:

   <iframe width="560" height="315" src="https://www.youtube.com/embed/2O89ByXY2I4" title="YouTube video player" frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
   </iframe>






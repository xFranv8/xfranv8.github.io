---
title: "Practice 3: Autolocalization"
categories:
  - Daily Log
tags:
  - github page
  - 3-dimensional reconstruction
  - gazebo
---

In this practice we were supposed to implement an autolocalization algorithm based on the Perspective N Points (PnP) algorithm. 
The algorithm is based on the following steps:
1. First, we need to calibrate our camera.

For this purpose, we will use the algorithm that we have already used in another subject of the master (3D Vision). This algorithm requieres a series of images from
a chessboard taken from different angles.

<figure class="half">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/calibration_pattern.svg" alt="" style="width:51%">
  <figcaption>Calibration pattern</figcaption>
</figure>

In this case, we've used 12 images taken with my phone camera, these images have a resolution of 
4032x3024 pixels. Here you can watch some examples of the images used for the calibration:

<figure class="half">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/IMG_4047.png" alt="" style="width:40%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/IMG_4049.png" alt="" style="width:40%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/IMG_4050.png" alt="" style="width:40%">
</figure>

The class _CalibrateCamera_ implemented in the **CalibrateCamera.py** file receives the directory all the calibration pattern
images, and with its method _calibrate_, we're able to obtain the camera matrix and the distortion coefficients.
We can set the parameter _check_ of this class to true if we want to check the calibration results. In this case, the method 
will show the projections of the coordinate axes in each of the images, as shown in the figure below:

<figure class="half">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/checking_calibration1.png" alt="" style="width:40%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/checking_calibration2.png" alt="" style="width:40%">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/checking_calibration3.png" alt="" style="width:40%">
</figure>

As we can see, the calibration results are quite good, so we can use them to obtain the camera pose.

2. Once we have our camera calibrated, we can start with the autolocalization algorithm. In order to do that, we need to generate
an aruco marker, which will be used as a reference in the autolocalization process. We can generate the marker with the _create_marker.py_
file, which will generate a 6x6 ArUco marker as the one shown below:

<figure class="half">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/autolocalization/marker.png" alt="" style="width:50%">
</figure>







